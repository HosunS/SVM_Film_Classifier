{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3b9094f-21c0-4683-b490-beb5ef42c23e",
   "metadata": {},
   "source": [
    "# CS178 Final Project\n",
    "## IMDB Reviews dataset\n",
    "### Students:\n",
    "Rebecca Park : 50269810\n",
    "Nick Hosun Song : 65482420"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3280e749-2ee9-40e6-bfe5-088812ff0a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import csr_matrix, vstack\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC , LinearSVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score , make_scorer\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.preprocessing import PolynomialFeatures,StandardScaler\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, GridSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "## conda install libsvm\n",
    "## conda install libsvm-python\n",
    "from libsvm.svmutil import svm_train, svm_predict, svm_problem, svm_parameter , svm_read_problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f885b952-75b6-4af0-ae1c-36994bd6876e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the vocab from given data will correlate to bagofwords(feat) file\n",
    "def load_vocab(vocab_file):\n",
    "    with open(vocab_file, 'r',encoding ='utf-8') as file:\n",
    "        vocab = file.read().splitlines()\n",
    "    return {index: word for index, word in enumerate(vocab)}\n",
    "\n",
    "vocab = load_vocab('imdb.vocab')\n",
    "def convert_to_csr(data, num_features):\n",
    "    row_ind = []\n",
    "    col_ind = []\n",
    "    data_values = []\n",
    "\n",
    "    for i, row in enumerate(data):\n",
    "        for feature_index, feature_value in row.items():\n",
    "            row_ind.append(i)\n",
    "            col_ind.append(feature_index)\n",
    "            data_values.append(feature_value)\n",
    "\n",
    "    return csr_matrix((data_values, (row_ind, col_ind)), shape=(len(data), num_features))\n",
    "\n",
    "num_features = len(vocab)\n",
    "\n",
    "y_train, X_train = svm_read_problem('train/labeledBow.feat')\n",
    "y_test, X_test = svm_read_problem('test/labeledBow.feat')\n",
    "\n",
    "X_train_csr = convert_to_csr(X_train,num_features)\n",
    "X_test_csr = convert_to_csr(X_test,num_features)\n",
    "\n",
    "#convert labels into binary format\n",
    "#positive = 1 (7 , 8 , 9 , 10)\n",
    "#negative = 0 (1, 2 , 3 , 4)\n",
    "y_train_binary = [1 if y > 6 else 0 for y in y_train]\n",
    "y_test_binary = [1 if y > 6 else 0 for y in y_test]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1257b8d6-bc25-4909-9974-36189e978fbd",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "We are focusing on creating a sentiment analysis which is a natural language processing task that tries to determine \n",
    "the emotional tone of a piece of text which in our case is an IMDB movie review. We will be classifying our data into \n",
    "positive (1) and negative (0) labels. Our data was provided to us in a .feat format so all we had to do for data preprocessing\n",
    "was convert the format into csr in order for us to be able to use Scikit_learn packages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580b1b38-c57d-4c6f-bde0-7cecd71778b8",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "The first model is Naive Bayes which assumes that all the features are independent of each other. Although we know that there are contextual relationships between words we use this as a baseline as Naive Bayes is a simple but somewhat effective model for text classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a20cddbf-8a05-4447-bc6a-fb5445a881a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Error Rate: 0.2177\n",
      "Cross-Validation F1 Score: 0.7656\n",
      "Test Error Rate: 0.1864\n",
      "Test F1 Score: 0.8010\n"
     ]
    }
   ],
   "source": [
    "# Assuming X_train_csr, y_train_binary, X_test_csr, y_test_binary are defined\n",
    "\n",
    "# Scoring metrics definitions for error rate and F1 score\n",
    "scoring_metrics = {\n",
    "    'error_rate': make_scorer(lambda y, y_pred, **kwargs: 1 - accuracy_score(y, y_pred)),\n",
    "    'f1': make_scorer(f1_score)\n",
    "}\n",
    "\n",
    "# Initialize Naive Bayes model\n",
    "model = MultinomialNB()\n",
    "\n",
    "# Perform 3-fold cross-validation and score the average of each metric\n",
    "cv_results = cross_validate(model, X_train_csr, y_train_binary, cv=3, scoring=scoring_metrics, n_jobs=-1)\n",
    "\n",
    "# Store cross-validation metrics\n",
    "cv_error_rate = np.mean(cv_results['test_error_rate'])\n",
    "cv_f1_score = np.mean(cv_results['test_f1'])\n",
    "\n",
    "# Train the model and evaluate on test set\n",
    "model.fit(X_train_csr, y_train_binary)\n",
    "y_pred = model.predict(X_test_csr)\n",
    "test_error_rate = 1 - accuracy_score(y_test_binary, y_pred)\n",
    "test_f1_score = f1_score(y_test_binary, y_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Cross-Validation Error Rate: {cv_error_rate:.4f}\")\n",
    "print(f\"Cross-Validation F1 Score: {cv_f1_score:.4f}\")\n",
    "print(f\"Test Error Rate: {test_error_rate:.4f}\")\n",
    "print(f\"Test F1 Score: {test_f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a75dc97-6ff1-4b0e-949c-573af7cc89b5",
   "metadata": {},
   "source": [
    "## Linear SVM\n",
    "The second type of model we chose was an SVM with a linear kernel. The linear kernel tends to work well when data is linearly separable in a feature space.\n",
    "The hyperparameter that we chose to tune was the regularization strength parameter (C). We chose to use L2 regularization as it doesn't force feature selection as aggressively as L1 regularization. Tuning the reglarization is done in order to prevent overfitting and improve the generalization of the model. The strength parameter C encourages the model to have smaller weights/coefficients for each of the features, which will make the model less sensitive to the noise in the training data. This will help us control the margin between the decision boundary and the training points. In using a Linear SVM we are aiming to find the decision boundary that maximizes the margin between the classes while also correctly classifying the training examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647d3624-4c43-4e8d-abf7-e17b7b4376c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C = 1e-05\n",
      "Cross-Validation Error Rate: 0.2402\n",
      "Cross-Validation F1 Score: 0.7581\n",
      "Test Error Rate: 0.2205\n",
      "Test F1 Score: 0.7795\n",
      "\n",
      "C = 0.0001\n",
      "Cross-Validation Error Rate: 0.1650\n",
      "Cross-Validation F1 Score: 0.8368\n",
      "Test Error Rate: 0.1490\n",
      "Test F1 Score: 0.8526\n",
      "\n",
      "C = 0.001\n",
      "Cross-Validation Error Rate: 0.1362\n",
      "Cross-Validation F1 Score: 0.8640\n",
      "Test Error Rate: 0.1174\n",
      "Test F1 Score: 0.8831\n",
      "\n",
      "C = 0.01\n",
      "Cross-Validation Error Rate: 0.1429\n",
      "Cross-Validation F1 Score: 0.8555\n",
      "Test Error Rate: 0.1222\n",
      "Test F1 Score: 0.8774\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing with linear kernel with different regularization strengths and cross-validation to test different metrics\n",
    "C_values = [0.00001, 0.0001, 0.001, 0.01, 0.1, 1]\n",
    "\n",
    "# Dictionaries to store cross-validation and test metrics (error_rate, f1)\n",
    "cv_metrics = {'error_rate': [], 'f1': []}\n",
    "test_metrics = {'error_rate': [], 'f1': []}\n",
    "\n",
    "# Loop through and create a model for each regularization strength\n",
    "for C in C_values:\n",
    "    model = LinearSVC(C=C, dual=False, max_iter=5000)\n",
    "\n",
    "    # For each regularization strength, perform 3-fold cross-validation and score the average of each metric\n",
    "    cv_results = cross_validate(model, X_train_csr, y_train_binary, cv=3, scoring=scoring_metrics,n_jobs=-1)\n",
    "    cv_metrics['error_rate'].append(np.mean(cv_results['test_error_rate']))\n",
    "    cv_metrics['f1'].append(np.mean(cv_results['test_f1']))\n",
    "\n",
    "    # Do a final testing with a simple train / test split\n",
    "    model.fit(X_train_csr, y_train_binary)\n",
    "    y_pred = model.predict(X_test_csr)\n",
    "    test_metrics['error_rate'].append(1 - accuracy_score(y_test_binary, y_pred))\n",
    "    test_metrics['f1'].append(f1_score(y_test_binary, y_pred))\n",
    "\n",
    "    # Print the results for each C value\n",
    "    print(f\"C = {C}\")\n",
    "    print(f\"Cross-Validation Error Rate: {cv_metrics['error_rate'][-1]:.4f}\")\n",
    "    print(f\"Cross-Validation F1 Score: {cv_metrics['f1'][-1]:.4f}\")\n",
    "    print(f\"Test Error Rate: {test_metrics['error_rate'][-1]:.4f}\")\n",
    "    print(f\"Test F1 Score: {test_metrics['f1'][-1]:.4f}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "def plot_metric(metric, title):\n",
    "    plt.plot(C_values, cv_metrics[metric], marker='o', color='b', label='CV ' + title)\n",
    "    plt.plot(C_values, test_metrics[metric], linestyle='--', marker='x', color='r', label='Test ' + title)\n",
    "    plt.xscale('log')\n",
    "    plt.xlabel('C (Regularization Strength)')\n",
    "    plt.ylabel(title)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "\n",
    "# Plot the metrics for error rate and F1 score\n",
    "plt.figure(figsize=(10, 4))\n",
    "plot_metric('error_rate', 'Error Rate')\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plot_metric('f1', 'F1 Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4130006e-2273-4719-85b6-d6ba80f99843",
   "metadata": {},
   "source": [
    "## Analysis of Linear SVM tuning\n",
    "In our analysis of the Linear SVM, after computing all of the linear models using the different Regularization strengths, we measure 4 different metrics.\n",
    "- Error Rate : the proportion of misclassified instances\n",
    "- Precision : proportion of models ability to correctly predict positive cases (minimize false positives)\n",
    "- Recall : proportion of true positive predictions out of all the actual positive cases (minimize false negatives)\n",
    "- F1 score: the mean between precision and recall finds the best model that captures the balance between minimizing both false positives and negatives.\n",
    "\n",
    "From the graphs shown above, we can clearly tell that when using a Linear SVM a regularization strength of .001 gives us the best overall performance. If we were to focus only on the precision metric we cold arge that choosing to go with .01 would be better, but for our dataset we want a good balance of both false positives and negatives. So if we choose based off the error rate as well as the F1 score we can confidently say that choosing the Linear SVM model with a regularization strength of .001 is the best choice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ed1c70-b529-46c7-abce-e0852ce60025",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, make_scorer\n",
    "from sklearn.model_selection import cross_validate\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define C and gamma values to test\n",
    "C_values = [0.00001, 0.0001, 0.001, 0.01, 0.1, 1]\n",
    "gamma_values = [0.001, 0.01, 0.1, 1]\n",
    "\n",
    "# Initialize dictionaries to store metrics\n",
    "cv_metrics_rbf = {'error_rate': [], 'f1': []}\n",
    "test_metrics_rbf = {'error_rate': [], 'f1': []}\n",
    "labels = []\n",
    "\n",
    "# Loop through C and gamma values\n",
    "for C in C_values:\n",
    "    for gamma in gamma_values:\n",
    "        model = Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('svc', SVC(C=C, kernel='rbf', gamma=gamma))\n",
    "        ])\n",
    "        # model = SVC(C=C, kernel='rbf', gamma=gamma)\n",
    "\n",
    "        # Perform cross-validation\n",
    "        cv_results = cross_validate(model, X_train_csr, y_train_binary, cv=3, scoring=scoring_metrics, n_jobs=-1)\n",
    "        cv_metrics_rbf['error_rate'].append(np.mean(cv_results['test_error_rate']))\n",
    "        cv_metrics_rbf['f1'].append(np.mean(cv_results['test_f1']))\n",
    "\n",
    "        # Fit the model and calculate test metrics\n",
    "        model.fit(X_train_csr, y_train_binary)\n",
    "        y_pred = model.predict(X_test_csr)\n",
    "        test_metrics_rbf['error_rate'].append(1 - accuracy_score(y_test_binary, y_pred))\n",
    "        test_metrics_rbf['f1'].append(f1_score(y_test_binary, y_pred))\n",
    "\n",
    "        # Save the label for plotting\n",
    "        labels.append(f\"C={C}, Î³={gamma}\")\n",
    "\n",
    "        # Print results\n",
    "        print(f\"C = {C}, gamma = {gamma}\")\n",
    "        print(f\"Cross-Validation Error Rate: {cv_metrics_rbf['error_rate'][-1]:.4f}\")\n",
    "        print(f\"Cross-Validation F1 Score: {cv_metrics_rbf['f1'][-1]:.4f}\")\n",
    "        print(f\"Test Error Rate: {test_metrics_rbf['error_rate'][-1]:.4f}\")\n",
    "        print(f\"Test F1 Score: {test_metrics_rbf['f1'][-1]:.4f}\")\n",
    "        print()\n",
    "\n",
    "# Function to plot metrics for RBF SVM\n",
    "def plot_metric_rbf(metric, title):\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.plot(labels, cv_metrics_rbf[metric], marker='o', color='b', label='CV ' + title)\n",
    "    plt.plot(labels, test_metrics_rbf[metric], linestyle='--', marker='x', color='r', label='Test ' + title)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.xlabel('C and Gamma Combinations')\n",
    "    plt.ylabel(title)\n",
    "    plt.title(f\"RBF SVM {title}\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Plotting the metrics for RBF SVM\n",
    "plot_metric_rbf('error_rate', 'Error Rate')\n",
    "plot_metric_rbf('f1', 'F1 Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff868df4-164b-4aa2-a487-e76a3150d65e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
